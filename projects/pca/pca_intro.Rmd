---
title: Introduction to Principal Component Analysis
author: Kenny Lov
output:
  md_document:
    variant: gfm
    
tables: yes
always_allow_html: yes

knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), 'index.md')) })

header-includes:
  - \usepackage{asmath}
---


```{r, echo = F}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```


# __Introduction to Principal Component Analysis__

<p style = 'text-align:center;'>
<em>Kenny Lov</em><br><br>
</p>

<p>
Ever been in a situation where you had to analyze a dataset with way too many variables? Perhaps you wanted to somehow plot your data in a 2-D or 3-D graph while preserving as much information as possible? Was there also high correlation among those variables? If you answered <b>yes</b> to all of these questions, then <i>Principal Components Analysis</i> is be a powerful dimensionality reduction tool that should be in your arsenal!
</p>


Quick overview of the technique:
<ol>
<li> Center the dataset \(X\) that has \(n\) observations and \(d\) variables </li>
<li> Compute the sample covariance matrix of \(X\), \(Q \in {\rm I\!R}^{d \times d}\) </li>
<li> Diagonalize the sample covariance matrix \(Q\) such that it has the form \(Q = PDP^T\), where \(P\) is an orthogonal matrix consisting of \(\begin{bmatrix} \mathbf{v_1}  \mathbf{v_2}  ...  \mathbf{v_d} \end{bmatrix}\) and \(D\) is a diagonal matrix such that Diag(\(D\)) \( = \lambda_1, \lambda_2, ..., \lambda_d\), where each \(\mathbf{v_i}\) is an eigenvector of \(Q\) that corresponds to its eigenvalue \(lambda_i\) </li>
<li> Decide the number of principal components to use (\(k\)), such that \(k \leq d\), and set \(P_k = \begin{bmatrix} \mathbf{v_1}  \mathbf{v_2}  ...  \mathbf{v_k} \end{bmatrix} \in {\rm I\!R}^{d \times k}\)
</li>
<li> Transform the original vectors \(X\) to \(Y\) by projecting onto the lower dimensional subspace spanned by the \(k\) eigenvectors by computing \(Y = XP_k \in {\rm I\!R}^{n \times k}\)</li>
<li> The columns of \(Y\) will be referred to as the principal components and have the nice property of orthogonality</li>
</ol>

```{r}

```


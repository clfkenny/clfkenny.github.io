---
title: Introduction to Principal Component Analysis
author: Kenny Lov
output:
  md_document:
    variant: gfm
    
tables: yes
always_allow_html: yes

knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), 'index.md')) })

header-includes:
  - \usepackage{asmath}
---


```{r, echo = F}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```


# __Introduction to Principal Component Analysis__

<p style = 'text-align:center;'>
<em>Kenny Lov</em><br><br>
</p>

<p>
Ever been in a situation where you had to analyze a dataset with way too many variables? Perhaps you wanted to somehow plot your data in a 2-D or 3-D graph while preserving as much information as possible? Was there also high correlation among those variables? If you answered <b>yes</b> to all of these questions, then <i>Principal Components Analysis</i> is be a powerful dimensionality reduction tool that should be in your arsenal!
</p>


Basic rundown of the algorithm:
<ol>
<li> Center the dataset \(X\) that has \(n\) observations and \(d\) variables </li>
<li> Compute the sample covariance matrix of \(X\), \(Q\) (size d x d) </li>
<li> Diagonalize sample covariance matrix \(Q\) such that it has the form \(Q = PDP^T\), where \(P\) is an orthogonal matrix consisting of \(\begin{bmatrix} v_1 & v_2 & ... & v_d \end{bmatrix}\) and \(D\) is a diagonal matrix such that Diag(\(D\)) \( = \lambda_1, \lambda_2, ..., \lambda_d\), where each \(v_i\) is an eigenvector of \(Q\) that corresponds to its eigenvalue \(lambda_i\) </li>
<li> 
</ol>

```{r}

```

